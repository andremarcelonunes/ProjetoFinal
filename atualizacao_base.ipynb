{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  DATA VISUALIZATION PROJECT - ALL ATHLETICS COMPETITIONS FROM 2010 TO 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORT LIBRARIES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-01T22:55:41.002440Z",
     "start_time": "2022-02-01T22:55:36.353077Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2\n",
      "  Downloading psycopg2-2.9.3-cp39-cp39-win_amd64.whl (1.2 MB)\n",
      "Installing collected packages: psycopg2\n",
      "Successfully installed psycopg2-2.9.3\n"
     ]
    }
   ],
   "source": [
    "!pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T03:21:03.258449Z",
     "start_time": "2022-02-19T03:20:52.239988Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting simplejson\n",
      "  Downloading simplejson-3.17.6-cp39-cp39-win_amd64.whl (75 kB)\n",
      "Installing collected packages: simplejson\n",
      "Successfully installed simplejson-3.17.6\n"
     ]
    }
   ],
   "source": [
    "!pip install -U simplejson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.1.3-py3-none-any.whl (968 kB)\n",
      "Collecting trio~=0.17\n",
      "  Downloading trio-0.20.0-py3-none-any.whl (359 kB)\n",
      "Collecting trio-websocket~=0.9\n",
      "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: urllib3[secure,socks]~=1.26 in c:\\users\\andre.nunes\\anaconda3\\lib\\site-packages (from selenium) (1.26.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\andre.nunes\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\andre.nunes\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Collecting outcome\n",
      "  Downloading outcome-1.1.0-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\andre.nunes\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.14.6)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\andre.nunes\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: idna in c:\\users\\andre.nunes\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.2)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\andre.nunes\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.2.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\andre.nunes\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.20)\n",
      "Collecting wsproto>=0.14\n",
      "  Downloading wsproto-1.1.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14 in c:\\users\\andre.nunes\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (21.0.0)\n",
      "Requirement already satisfied: cryptography>=1.3.4 in c:\\users\\andre.nunes\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (3.4.8)\n",
      "Requirement already satisfied: certifi in c:\\users\\andre.nunes\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (2021.10.8)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\andre.nunes\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\andre.nunes\\anaconda3\\lib\\site-packages (from pyOpenSSL>=0.14->urllib3[secure,socks]~=1.26->selenium) (1.16.0)\n",
      "Collecting h11<1,>=0.9.0\n",
      "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: outcome, h11, wsproto, trio, trio-websocket, selenium\n",
      "Successfully installed h11-0.13.0 outcome-1.1.0 selenium-4.1.3 trio-0.20.0 trio-websocket-0.9.2 wsproto-1.1.0\n",
      "Collecting webdriver-manager\n",
      "  Downloading webdriver_manager-3.5.4-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\andre.nunes\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.26.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\andre.nunes\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\andre.nunes\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\andre.nunes\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\andre.nunes\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2.0.4)\n",
      "Installing collected packages: webdriver-manager\n",
      "Successfully installed webdriver-manager-3.5.4\n"
     ]
    }
   ],
   "source": [
    "!pip3 install selenium\n",
    "!pip3 install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-20T03:01:41.655982Z",
     "start_time": "2022-02-20T03:01:41.638033Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import psycopg2\n",
    "from psycopg2.errors import UniqueViolation\n",
    "from psycopg2.errors import SyntaxError\n",
    "from sqlalchemy import create_engine\n",
    "import ssl\n",
    "from simplejson import JSONDecodeError\n",
    "from tqdm import tqdm\n",
    "from requests.exceptions import MissingSchema\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INITIAL PARAMETERS FOR PROCESSING DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-20T01:07:42.349229Z",
     "start_time": "2022-02-20T01:07:42.338261Z"
    }
   },
   "outputs": [],
   "source": [
    "year= 2021 # ALL FUNCTIONS ARE BASED ON YEAR THAT WILL BE PROCESSED, SO ALWAYS CHECK THIS BEFORE EXECUTING SOME CODE BELLOW\n",
    "token_wheter = open('apikey.txt', 'r').read()\n",
    "user_db = open('user_db.txt', 'r').read()\n",
    "pass_db = open('pass_db.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNCTIONS USED BY CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-20T02:13:44.989591Z",
     "start_time": "2022-02-20T02:13:44.955706Z"
    }
   },
   "outputs": [],
   "source": [
    "def getcomp(year, pag=1):\n",
    "    '''\n",
    "    year is the year when competition was done\n",
    "\n",
    "    Its return is a object soup. It´s a kind of html conteiner based an answer from server\n",
    "    or can be a integer -1 when there is no information for that year\n",
    "\n",
    "\n",
    "    '''  \n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "    get_html = requests.get(f'https://worldathletics.org/records/competition-performance-rankings?type=2&year={year}&sortBy=score&page={pag}')\n",
    "    html = get_html.content\n",
    "    soup = BeautifulSoup(html)\n",
    "    list_results = soup.find_all('h2', attrs = {'class' : 'no-results'})\n",
    "    if len(list_results) > 0:\n",
    "        return -1\n",
    "    return [soup, pag]\n",
    "\n",
    "\n",
    "def atualiza_prova_compet(cursor, cod_prova, cod_competicao, score_prova_competicao,data_prova_competicao):\n",
    "    \n",
    "    '''\n",
    "    This function creates e correlates proves and competitions\n",
    "    '''  \n",
    "    try: \n",
    "        print(f'Inserindo competicao {cod_competicao} e prova {cod_prova}')\n",
    "        cursor.execute(f\"INSERT INTO tb_prova_competicao (cod_prova, cod_competicao, score_prova_competicao,data_prova_competicao)\" \\\n",
    "                       f\"VALUES ({cod_prova}, {cod_competicao[0]}, {score_prova_competicao},'{data_prova_competicao}')\")\n",
    "\n",
    "    except UniqueViolation:\n",
    "        print (\"prova ja existe na competicao\")\n",
    "        \n",
    "                    \n",
    "def atualiza_atleta_prova_competicao(cursor,cod_prova, cod_competicao, lista_tb_atleta_competicao):\n",
    "    '''\n",
    "    This function find each atletes for each  prove and insert it on prove\n",
    "\n",
    "\n",
    "    '''  \n",
    "    cursor.execute(f\"select cod_prova_competicao from tb_prova_competicao where cod_prova = {cod_prova} and cod_competicao = {cod_competicao[0]}\")\n",
    "    cod_prova_competicao = cursor.fetchone()[0] \n",
    "    for x in lista_tb_atleta_competicao:\n",
    "        try:\n",
    "            print(f'Inserindo atleta {x[0]}')\n",
    "            cursor.execute(f\"INSERT INTO tb_atleta_prova_competicao (cod_atleta, cod_prova_competicao, result, score)\" \\\n",
    "                         f\"VALUES ({x[0]}, {cod_prova_competicao}, '{x[1]}', {x[2]})\")\n",
    "        except UniqueViolation:\n",
    "            print (\"atleta ja existe na prova\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "def atualiza_atleta(cursor,cod_atleta, lista_tb_atleta_competicao):\n",
    "    '''\n",
    "    This function insert atlete code in table athete\n",
    "    '''  \n",
    "    for x in lista_tb_atleta_competicao:\n",
    "        try:\n",
    "            print(f'Inserindo atleta na tb_atleta {x[0]}')\n",
    "            cursor.execute(f\"INSERT INTO tb_atleta (cod_atleta)\" \\\n",
    "                         f\"VALUES ({x[0]})\")\n",
    "        except UniqueViolation:\n",
    "            print (\"atleta ja existe na tb_atleta\")\n",
    "            \n",
    "            \n",
    "            \n",
    "def update_atleta(cursor,cod_atleta,url_atleta,pais_atleta,nome_atleta):\n",
    "    \n",
    "    '''\n",
    "    This function update  atlete information  on table athete\n",
    "    '''  \n",
    "   \n",
    "        \n",
    "    cursor.execute(f\"UPDATE tb_atleta SET url_atleta = '{url_atleta}', \" \\\n",
    "                         f\"pais_atleta = '{pais_atleta}', nome_atleta = '{nome_atleta}' WHERE cod_atleta = {cod_atleta}\")\n",
    "    print (f\"atleta {cod_atleta} atualizado\")\n",
    "            \n",
    "def update_wheter(cursor,data_prova_competicao, pais_competicao, cidade_competicao, json_resp):\n",
    "    '''\n",
    "    This function update  weather information to every day thats occurs proves\n",
    "    '''\n",
    "    try:\n",
    "        cursor.execute(f\"INSERT INTO tb_wheter (data_prova_competicao, pais_competicao, cidade_competicao, info_wheter)\" \\\n",
    "                         f\"VALUES ('{data_prova_competicao}', '{pais_competicao}', '{cidade_competicao}', '{json_resp}')\")\n",
    "    except UniqueViolation:\n",
    "            print (\"condições climaticas já existem na base\")\n",
    "            \n",
    "            \n",
    "            \n",
    "def connect_db():\n",
    "    '''\n",
    "    This function connect data base\n",
    "    '''  \n",
    "   \n",
    "    import psycopg2\n",
    "    from psycopg2.errors import UniqueViolation\n",
    "    from psycopg2.errors import SyntaxError\n",
    "    connection = psycopg2.connect(user=user_db,\n",
    "                                  password=pass_db,\n",
    "                                  host=\"127.0.0.1\",\n",
    "                                  port=\"5432\",\n",
    "                                  database=\"postgres\",\n",
    "                                  options=\"-c search_path=db_atl\")\n",
    "\n",
    "\n",
    "    connection.autocommit = True\n",
    "    return connection.cursor()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def validate(date_text):\n",
    "    from datetime import datetime\n",
    "    \n",
    "    try:\n",
    "        datetime.strptime(data_formato, '%d-%b-%Y')\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insertions on tb_competicao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-13T18:43:10.181389Z",
     "start_time": "2022-02-13T18:42:50.025022Z"
    }
   },
   "outputs": [],
   "source": [
    "cursor = connect_db()\n",
    "contador = 0\n",
    "list_results = []\n",
    "soup = getcomp(year,1)[0]\n",
    "pagina = getcomp(year,1)[1]   \n",
    "last_page = int(soup.find_all('a', attrs = {'class' : 'btn--pagination btn--pag-last pag-control'})[0]['data-page'])\n",
    "list_competicao = soup.find_all('tbody')[0].find_all('tr')\n",
    "\n",
    "if last_page > 0:\n",
    "    for pagina_int in range(last_page):\n",
    "        pagina_int += 1\n",
    "        soup = getcomp(year,pagina_int)[0]\n",
    "        pagina = getcomp(year,pagina_int)[1]   \n",
    "        list_competicao = soup.find_all('tbody')[0].find_all('tr')\n",
    "    \n",
    "        time.sleep(5)\n",
    "\n",
    "        for line in range(len(list_competicao)-1):\n",
    "            try:\n",
    "                cod_competicao = re.findall(r'\\d+',list_competicao[line]['data-href'])[0]\n",
    "                place = list_competicao[line].find_all('td')[0].text.strip()\n",
    "                #  competition = list_competicao[line].find_all('td')[1].text.strip()\n",
    "            \n",
    "                tamanho = len(list_competicao[line].find_all('td')[1].text.strip().split(\",\"))\n",
    "            \n",
    "                lista_nome_competicao = list_competicao[line].find_all('td')[1].text.strip().split(\",\")\n",
    "                nome_competicao = \" \".join(re.findall(\"[a-zA-Z]+\", lista_nome_competicao[0].strip()))\n",
    "                cidade_competicao =  \" \".join(re.findall(\"[a-zA-Z]+\", lista_nome_competicao[len(lista_nome_competicao)-1].strip()))\n",
    "                   \n",
    "         \n",
    "                country = list_competicao[line].find_all('td')[2].text.strip()\n",
    "                Start_date = list_competicao[line].find_all('td')[3].text.strip()\n",
    "                end_date = list_competicao[line].find_all('td')[4].text.strip()\n",
    "                part_score = list_competicao[line].find_all('td')[5].text.strip()\n",
    "                ps_place = list_competicao[line].find_all('td')[6].text.strip()\n",
    "                result_score = list_competicao[line].find_all('td')[7].text.strip()\n",
    "                rs_place = list_competicao[line].find_all('td')[8].text.strip()\n",
    "       \n",
    "                if pagina == 1:\n",
    "                   comp_score = list_competicao[line].find_all('td')[10].text.strip()\n",
    "                else:\n",
    "                    comp_score = list_competicao[line].find_all('td')[9].text.strip()\n",
    "             \n",
    "                list_results.append((cod_competicao, nome_competicao, comp_score, part_score, result_score, Start_date, end_date,\n",
    "                                  cidade_competicao, country, ps_place, rs_place ))\n",
    "            \n",
    "            \n",
    "                sql_update = f\"INSERT INTO tb_competicao (cod_competicao, nome_competição, score_competicao,\" \\\n",
    "                    f\"score_participacao_competicao, score_result_competicao, data_inicial_competicao,\" \\\n",
    "                    f\"data_final_competicao, cidade_competicao, pais_competicao,\" \\\n",
    "                    f\"ps_place_competicao, rs_place_competicao)\" \\\n",
    "                    f\" VALUES ({cod_competicao},'{nome_competicao}','{comp_score}','{part_score}','{result_score}','{Start_date}','{end_date}', \"\\\n",
    "                    f\"'{cidade_competicao}','{country}','{ps_place}','{rs_place}')\"\n",
    "                contador += 1  \n",
    "                print (f'Executando Query {contador} executada')\n",
    "                cursor.execute(sql_update)\n",
    "        \n",
    "           # except UniqueViolation:\n",
    "           #     print(\"Registro encontrado\")\n",
    "           #     continue     ''' \n",
    "            \n",
    "            except (Exception, psycopg2.DatabaseError) as error:\n",
    "                print(\"Error in transction Reverting all other operations of a transction \", error)\n",
    "    \n",
    "cursor.close()\n",
    "print(\"Transaction completed successfully \")\n",
    "\n",
    "\n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-03T03:40:37.772586Z",
     "start_time": "2022-02-03T03:40:37.761616Z"
    }
   },
   "source": [
    "## Insertions on other tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-20T00:56:17.149675Z",
     "start_time": "2022-02-20T00:27:59.201464Z"
    }
   },
   "outputs": [],
   "source": [
    "cursor = connect_db()\n",
    "\n",
    "cursor.execute(f\"SELECT cod_competicao  FROM tb_competicao WHERE date_part('year', data_final_competicao) = {year};\")\n",
    "               \n",
    "list_cod_comp = cursor.fetchall()   \n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "lista_full = []\n",
    "\n",
    "for compt in list_cod_comp:\n",
    "    get_html = requests.get(f'https://worldathletics.org/records/competition-performance-rankings/ranking/{compt[0]}')\n",
    "    print (f'https://worldathletics.org/records/competition-performance-rankings/ranking/{compt[0]}')\n",
    "    html = get_html.content\n",
    "    soup = BeautifulSoup(html)\n",
    "    list_results = soup.find_all('div', attrs = {'class' : 'cpr-result-score'})\n",
    "\n",
    "    list_competicao = soup.find_all('tbody')[0].find_all('tr')\n",
    "\n",
    "    for i in list_results:\n",
    "        try:\n",
    "            prova = i.find_all('h3')[0].text.split(\"'s\")\n",
    "            print (f'Prova {prova[0]} na modalidade {prova[1]}')\n",
    "            score_prova =i.find_all('p')[0].text.split(\":\")\n",
    "            cursor.execute(f\"INSERT INTO tb_prova (nome_prova, genero_prova) VALUES ('{prova[1]}', '{prova[0]}')\")\n",
    "            \n",
    "            cursor.execute(f\"SELECT cod_prova  FROM tb_prova WHERE nome_prova='{prova[1]}' and genero_prova='{prova[0]}';\")\n",
    "               \n",
    "            cod_prova = cursor.fetchone()        \n",
    "            print (f'codigo  da prova {cod_prova[0]}')         \n",
    "            lista_tb_atleta_competicao = []\n",
    "            for x in i.find_all('tbody')[0].find_all('tr'):\n",
    "                score = x.find_all('td')[0].text\n",
    "                result = x.find_all('td')[1].text\n",
    "                date = x.find_all('td')[2].text\n",
    "                cod_atleta = re.findall(r'\\d+', x.find_all('td')[3].find_all('span')[0]['data-href'])[0]\n",
    "                lista_full.append((cod_prova[0],score_prova[1].strip(), score.strip(), result.strip(), date.strip(), cod_atleta.strip()))    \n",
    "                lista_tb_atleta_competicao.append([cod_atleta,result,score])\n",
    "            atualiza_prova_compet(cursor, cod_prova[0],compt,score_prova[1], date.strip())\n",
    "            atualiza_atleta(cursor,cod_atleta, lista_tb_atleta_competicao)\n",
    "            atualiza_atleta_prova_competicao(cursor,cod_prova[0],compt,lista_tb_atleta_competicao)\n",
    "        \n",
    "        except IndexError:\n",
    "            break\n",
    "        except UniqueViolation:\n",
    "            print (\"Prova ja existe\")\n",
    "            cursor.execute(f\"SELECT cod_prova  FROM tb_prova WHERE nome_prova='{prova[1]}' and genero_prova='{prova[0]}';\")\n",
    "            cod_prova = cursor.fetchone()        \n",
    "            print (f'codigo  da prova {cod_prova[0]}')         \n",
    "            lista_tb_atleta_competicao = []\n",
    "            for x in i.find_all('tbody')[0].find_all('tr'):\n",
    "                try:\n",
    "                    score = x.find_all('td')[0].text\n",
    "                    result = x.find_all('td')[1].text\n",
    "                    date = x.find_all('td')[2].text\n",
    "                    cod_atleta = re.findall(r'\\d+', x.find_all('td')[3].find_all('span')[0]['data-href'])[0]\n",
    "                    lista_full.append((cod_prova[0],score_prova[1].strip(), score.strip(), result.strip(), date.strip(), cod_atleta.strip()))   \n",
    "                    lista_tb_atleta_competicao.append([cod_atleta,result,score])\n",
    "                except IndexError:\n",
    "                    break\n",
    "            \n",
    "            atualiza_prova_compet(cursor, cod_prova[0],compt,score_prova[1], date.strip())\n",
    "            atualiza_atleta(cursor,cod_atleta, lista_tb_atleta_competicao)\n",
    "            atualiza_atleta_prova_competicao(cursor,cod_prova[0],compt,lista_tb_atleta_competicao)\n",
    "            continue\n",
    "cursor.close()\n",
    "print(\"Transaction completed successfully \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating climate conditions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-20T01:19:22.922314Z",
     "start_time": "2022-02-20T01:07:52.813748Z"
    }
   },
   "outputs": [],
   "source": [
    "cursor = connect_db()\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "count_query = 0\n",
    "\n",
    "cursor.execute(f\"select distinct * from cidades where  date_part('year', data_prova_competicao) ={year}\")\n",
    "\n",
    "             \n",
    "list_data = cursor.fetchall()   \n",
    "\n",
    "\n",
    "for i in list_data:\n",
    "    cidade_competicao = [i][0][0]\n",
    "    pais_competicao = [i][0][1]\n",
    "    data_prova_competicao = [i][0][2]\n",
    "    cursor.execute(f\"select count (*) from tb_wheter where  cidade_competicao like '{cidade_competicao}' and \"\\\n",
    "               f\"pais_competicao like '{pais_competicao}' and data_prova_competicao = '{data_prova_competicao }'\")\n",
    "    list_find_reg = cursor.fetchall()   \n",
    "\n",
    "    if list_find_reg[0][0] != 1:\n",
    "        try:\n",
    "alCrossingWebServices/rest/services/timeline/{cidade_competicao}%2C%20{pais_competicao}/{data_prova_competicao}/{data_prova_competicao}?            resp_json = requests.get(f'https://weather.visualcrossing.com/VisuunitGroup=metric&include=days&key={token_wheter}&contentType=json')\n",
    "            if resp_json.status_code == 401:\n",
    "                print (f\"Vixe, chegou no limite de hoje, foram {count_query} queries\")\n",
    "                break\n",
    "            else:\n",
    "                count_query += 1\n",
    "                update_wheter(cursor, data_prova_competicao, pais_competicao, cidade_competicao, json.dumps(resp_json.json()))\n",
    "        except  JSONDecodeError:\n",
    "                print (f\"Erro na cidade {cidade_competicao} pais {pais_competicao}\")\n",
    "                resp_json = requests.get(f'https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline/{pais_competicao}/{data_prova_competicao}/{data_prova_competicao}?unitGroup=metric&include=days&key={token_wheter}&contentType=json')\n",
    "                count_query += 1\n",
    "                update_wheter(cursor, data_prova_competicao, pais_competicao, cidade_competicao, json.dumps(resp_json.json()))\n",
    "               \n",
    "                continue\n",
    "                \n",
    "        except SyntaxError:\n",
    "            try:\n",
    "                print (f\"Erro na cidade {cidade_competicao} pais {pais_competicao}\")\n",
    "                resp_json = requests.get(f'https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline/{pais_competicao}/{data_prova_competicao}/{data_prova_competicao}?unitGroup=metric&include=days&key={token_wheter}&contentType=json')\n",
    "                count_query += 1\n",
    "                update_wheter(cursor, data_prova_competicao, pais_competicao, cidade_competicao, json.dumps(resp_json.json()))\n",
    "            except SyntaxError:  \n",
    "                continue\n",
    "cursor.close()    \n",
    "print (\"fim do processamento\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating athletes information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-18T22:19:00.321142Z",
     "start_time": "2022-02-18T21:44:00.510565Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "cursor = connect_db()\n",
    "\n",
    "cursor.execute(f\"SELECT cod_competicao  FROM tb_competicao WHERE date_part('year', data_final_competicao) = {year};\")\n",
    "               \n",
    "list_cod_comp = cursor.fetchall()   \n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "lista_full = []\n",
    "\n",
    "for compt in list_cod_comp:\n",
    "    get_html = requests.get(f'https://worldathletics.org/records/competition-performance-rankings/ranking/{compt[0]}')\n",
    "    print (f'https://worldathletics.org/records/competition-performance-rankings/ranking/{compt[0]}')\n",
    "    html = get_html.content\n",
    "    soup = BeautifulSoup(html)\n",
    "    list_results = soup.find_all('div', attrs = {'class' : 'cpr-result-score'})\n",
    "\n",
    "    list_competicao = soup.find_all('tbody')[0].find_all('tr')\n",
    "    \n",
    "   \n",
    "    lista_tb_atleta_competicao = []   \n",
    "    \n",
    "\n",
    "    for i in list_results:\n",
    "        \n",
    "       \n",
    "            \n",
    "            for x in i.find_all('tbody')[0].find_all('tr'):\n",
    "                try:\n",
    "                        cod_atleta = re.findall(r'\\d+', x.find_all('td')[3].find_all('span')[0]['data-href'])[0]\n",
    "                        uri_atleta = x.find_all('td')[3].find_all('span')[0]['data-href']\n",
    "                        url_atleta =  'https://worldathletics.org' + uri_atleta\n",
    "                        pais_atleta =  uri_atleta.split('/')[2]\n",
    "                        nome_atleta =  uri_atleta.split('/')[3]\n",
    "                        if cod_atleta in lista_tb_atleta_competicao:\n",
    "                            print (\"ja atualizou\")\n",
    "                        else:\n",
    "                            update_atleta(cursor, cod_atleta,url_atleta,pais_atleta,nome_atleta)\n",
    "                            lista_tb_atleta_competicao.append(cod_atleta)\n",
    "                               \n",
    "                except IndexError:\n",
    "                    break\n",
    "                    \n",
    "            \n",
    "cursor.close()\n",
    "print(\"Transaction completed successfully \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-17T03:18:43.479848Z",
     "start_time": "2022-02-17T03:18:43.379119Z"
    }
   },
   "source": [
    "## Update atlhetes information on competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-20T10:43:08.462902Z",
     "start_time": "2022-02-20T03:01:51.596356Z"
    }
   },
   "outputs": [],
   "source": [
    "cursor = connect_db()\n",
    "\n",
    "cursor.execute(f\"select distinct AT.cod_atleta, AT.url_atleta from db_atl.tb_competicao as T, db_atl.tb_prova_competicao as C,\" \\\n",
    "               f\"db_atl.tb_prova as P, tb_atleta_prova_competicao as A, tb_atleta as AT \" \\\n",
    "               f\" where  T.cod_competicao = C.cod_competicao and C.cod_prova_competicao = A.cod_prova_competicao and AT.nascimento_atleta IS NULL and \" \\\n",
    "               f\"C.cod_prova = P.cod_prova and P.nome_prova like '_100 Metres_%'  and AT.cod_atleta = A.cod_atleta and C.data_prova_competicao >= '2010-01-01 00:00:00';\" )\n",
    "       \n",
    "       \n",
    "list_cod_url = cursor.fetchall()   \n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "print (f'atualizando {len(list_cod_url)} atletas' )\n",
    "\n",
    "for compt in list_cod_url:\n",
    "    try:\n",
    "        get_html = requests.get(compt[1])\n",
    "        print (compt[1])\n",
    "        html = get_html.content\n",
    "        soup = BeautifulSoup(html)\n",
    "        date_born = soup.find_all('div', attrs = {'class' : 'profileBasicInfo_statValue__IXJTW'})[1].text.strip()\n",
    "        idade = soup.find_all('div', attrs = {'class' : 'profileBasicInfo_statValue__IXJTW'})[3].text.strip()\n",
    "        lista_dados = soup.find_all('div', attrs = {'class' : 'profileBasicInfo_statValue__IXJTW'})\n",
    "        if  len(lista_dados) == 4:\n",
    "            print (\"idade eh \", idade)\n",
    "        data_formato = date_born.replace(\" \", \"-\")\n",
    "        if  validate(data_formato):\n",
    "            print (compt[1], compt[0], data_formato)\n",
    "           # cursor.execute(f\"UPDATE tb_atleta SET nascimento_atleta = '{data_formato}' \" \\\n",
    "          #              f\"WHERE cod_atleta = {compt[0]}\")\n",
    "        else:\n",
    "            print (\"sem data de nascimnto\")\n",
    "            print (compt[1], compt[0], data_formato)\n",
    "    except IndexError:\n",
    "        continue\n",
    "    except MissingSchema:\n",
    "        continue\n",
    "\n",
    "cursor.close()\n",
    "print(\"Transaction completed successfully \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atualizando 66935 atletas\n",
      "https://worldathletics.org/athletes/spain/luis-wee-palacios-014164681\n",
      "idade eh  34\n",
      "https://worldathletics.org/athletes/spain/luis-wee-palacios-014164681\n",
      "idade eh  34\n",
      "https://worldathletics.org/athletes/spain/javier-sanz-aleman-014165016\n",
      "idade eh  32\n",
      "https://worldathletics.org/athletes/spain/javier-sanz-aleman-014165016\n",
      "idade eh  32\n",
      "https://worldathletics.org/athletes/spain/javier-sanz-aleman-014165016\n",
      "idade eh  32\n",
      "https://worldathletics.org/athletes/spain/javier-sanz-aleman-014165016\n",
      "idade eh  32\n",
      "https://worldathletics.org/athletes/spain/javier-sanz-aleman-014165016\n",
      "idade eh  32\n",
      "https://worldathletics.org/athletes/spain/javier-sanz-aleman-014165016\n",
      "idade eh  32\n",
      "https://worldathletics.org/athletes/spain/javier-sanz-aleman-014165016\n",
      "idade eh  32\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ANDRE~1.NUN\\AppData\\Local\\Temp/ipykernel_114744/3996107136.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcompt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist_cod_url\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mget_html\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcompt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_html\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m     \"\"\"\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    540\u001b[0m         }\n\u001b[0;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    675\u001b[0m             \u001b[1;31m# Redirect resolving generator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m             \u001b[0mgen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresolve_redirects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 677\u001b[1;33m             \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mresp\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    678\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m             \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    675\u001b[0m             \u001b[1;31m# Redirect resolving generator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m             \u001b[0mgen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresolve_redirects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 677\u001b[1;33m             \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mresp\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    678\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m             \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mresolve_redirects\u001b[1;34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m             \u001b[1;31m# Rebuild auth and proxy information.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m             \u001b[0mproxies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrebuild_proxies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepared_request\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrebuild_auth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepared_request\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrebuild_proxies\u001b[1;34m(self, prepared_request, proxies)\u001b[0m\n\u001b[0;32m    292\u001b[0m         \u001b[0mbypass_proxy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshould_bypass_proxies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_proxy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mno_proxy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrust_env\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mbypass_proxy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m             \u001b[0menviron_proxies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_environ_proxies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_proxy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mno_proxy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m             \u001b[0mproxy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menviron_proxies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscheme\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menviron_proxies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'all'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\requests\\utils.py\u001b[0m in \u001b[0;36mget_environ_proxies\u001b[1;34m(url, no_proxy)\u001b[0m\n\u001b[0;32m    798\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 800\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetproxies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    801\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mgetproxies\u001b[1;34m()\u001b[0m\n\u001b[0;32m   2704\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2705\u001b[0m         \"\"\"\n\u001b[1;32m-> 2706\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetproxies_environment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mgetproxies_registry\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2708\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mproxy_bypass_registry\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mgetproxies_registry\u001b[1;34m()\u001b[0m\n\u001b[0;32m   2665\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mproxies\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2666\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2667\u001b[1;33m             internetSettings = winreg.OpenKey(winreg.HKEY_CURRENT_USER,\n\u001b[0m\u001b[0;32m   2668\u001b[0m                 r'Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings')\n\u001b[0;32m   2669\u001b[0m             proxyEnable = winreg.QueryValueEx(internetSettings,\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cursor = connect_db()\n",
    "\n",
    "cursor.execute(f\"SELECT data_prova_competicao, cod_atleta, url_atleta  from db_atl.db_provas_100 order by cod_atleta\")           \n",
    "list_cod_url = cursor.fetchall()   \n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "print (f'atualizando {len(list_cod_url)} atletas' )\n",
    "\n",
    "for compt in list_cod_url:\n",
    "    try:\n",
    "        get_html = requests.get(compt[2])\n",
    "        print (compt[2])\n",
    "        html = get_html.content\n",
    "        soup = BeautifulSoup(html)\n",
    "        lista_dados = soup.find_all('div', attrs = {'class' : 'profileBasicInfo_statValue__IXJTW'})\n",
    "        \n",
    "        \n",
    "        # clicar em results com selenium\n",
    "                 #<div class=\"profileStatistics_tab__1Blal\">Results</div>\n",
    "        \n",
    "        # selecionar o ano data_prova_competicao  selenium\n",
    "        '''  <select id=\"resultsYearSelect\">\n",
    "                  <option value=\"2021\">2021</option>\n",
    "                  <option value=\"2020\">2020</option>\n",
    "                  <option value=\"2019\">2019</option>\n",
    "                  <option value=\"2018\">2018</option>\n",
    "                  <option value=\"2017\">2017</option>\n",
    "                  <option value=\"2016\">2016</option>\n",
    "                  <option value=\"2015\">2015</option>\n",
    "                  <option value=\"2014\">2014</option>\n",
    "                  <option value=\"2013\">2013</option>\n",
    "                  <option value=\"2012\">2012</option>\n",
    "                  <option value=\"2011\">2011</option>\n",
    "                  <option value=\"2010\">2010</option>\n",
    "                  <option value=\"2009\">2009</option>\n",
    "                  <option value=\"2008\">2008</option>\n",
    "                  <option value=\"2007\">2007</option>\n",
    "                  </select>'''\n",
    "        # ler tabela dos resultados \n",
    "                # <tbody class=\"profileStatistics_tableBody__1w5O9\">     \n",
    "        \n",
    "        # guardar numa tabela nova \n",
    "                   \n",
    "         \n",
    "        \n",
    "        if  len(lista_dados) == 4:\n",
    "            idade = soup.find_all('div', attrs = {'class' : 'profileBasicInfo_statValue__IXJTW'})[3].text.strip()\n",
    "            print (\"idade eh \", idade)\n",
    "            list_result = soup.find('div')[0].find_all('tr')   \n",
    "            \n",
    "            print (list_result)\n",
    "            # Fazer aqui ano atual - idade \n",
    "            # inserir na base\n",
    "         \n",
    "        else:\n",
    "            print (\"sem idade\")\n",
    "            # inserir NaN???\n",
    "            \n",
    "         \n",
    "       \n",
    "    except IndexError:\n",
    "        continue\n",
    "    except MissingSchema:\n",
    "        continue\n",
    "\n",
    "cursor.close()\n",
    "print(\"Transaction completed successfully \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<div class=\"profileStatistics_contentInner__3to5y\"></div>, <div class=\"profileStatistics_contentInner__3to5y\"></div>]\n"
     ]
    }
   ],
   "source": [
    "print (list_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<tbody class=\"profileStatistics_tableBody__1w5O9\"><tr>\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_competicao = soup.find_all('tbody')[0].find_all('tr')\n",
    "\n",
    "if last_page > 0:\n",
    "    for pagina_int in range(last_page):\n",
    "        pagina_int += 1\n",
    "        soup = getcomp(year,pagina_int)[0]\n",
    "        pagina = getcomp(year,pagina_int)[1]   \n",
    "        list_competicao = soup.find_all('tbody')[0].find_all('tr')\n",
    "    \n",
    "        time.sleep(5)\n",
    "\n",
    "        for line in range(len(list_competicao)-1):\n",
    "            try:\n",
    "                cod_competicao = re.findall(r'\\d+',list_competicao[line]['data-href'])[0]\n",
    "                place = list_competicao[line].find_all('td')[0].text.strip()\n",
    "                #  competition = list_competicao[line].find_all('td')[1].text.strip()\n",
    "            \n",
    "                tamanho = len(list_competicao[line].find_all('td')[1].text.strip().split(\",\"))\n",
    "            \n",
    "                lista_nome_competicao = list_competicao[line].find_all('td')[1].text.strip().split(\",\")\n",
    "                nome_competicao = \" \".join(re.findall(\"[a-zA-Z]+\", lista_nome_competicao[0].strip()))\n",
    "                cidade_competicao =  \" \".join(re.findall(\"[a-zA-Z]+\", lista_nome_competicao[len(lista_nome_competicao)-1].strip()))\n",
    "                   \n",
    "         \n",
    "                country = list_competicao[line].find_all('td')[2].text.strip()\n",
    "                Start_date = list_competicao[line].find_all('td')[3].text.strip()\n",
    "                end_date = list_competicao[line].find_all('td')[4].text.strip()\n",
    "                part_score = list_competicao[line].find_all('td')[5].text.strip()\n",
    "                ps_place = list_competicao[line].find_all('td')[6].text.strip()\n",
    "                result_score = list_competicao[line].find_all('td')[7].text.strip()\n",
    "                rs_place = list_competicao[line].find_all('td')[8].text.strip()\n",
    "       \n",
    "                if pagina == 1:\n",
    "                   comp_score = list_competicao[line].find_all('td')[10].text.strip()\n",
    "                else:\n",
    "                    comp_score = list_competicao[line].find_all('td')[9].text.strip()\n",
    "             \n",
    "                list_results.append((cod_competicao, nome_competicao, comp_score, part_score, result_score, Start_date, end_date,\n",
    "                                  cidade_competicao, country, ps_place, rs_place ))\n",
    "            \n",
    "            \n",
    "                sql_update = f\"INSERT INTO tb_competicao (cod_competicao, nome_competição, score_competicao,\" \\\n",
    "                    f\"score_participacao_competicao, score_result_competicao, data_inicial_competicao,\" \\\n",
    "                    f\"data_final_competicao, cidade_competicao, pais_competicao,\" \\\n",
    "                    f\"ps_place_competicao, rs_place_competicao)\" \\\n",
    "                    f\" VALUES ({cod_competicao},'{nome_competicao}','{comp_score}','{part_score}','{result_score}','{Start_date}','{end_date}', \"\\\n",
    "                    f\"'{cidade_competicao}','{country}','{ps_place}','{rs_place}')\"\n",
    "                contador += 1  \n",
    "                print (f'Executando Query {contador} executada')\n",
    "                cursor.execute(sql_update)\n",
    "        \n",
    "           # except UniqueViolation:\n",
    "           #     print(\"Registro encontrado\")\n",
    "           #     continue     ''' \n",
    "            \n",
    "            except (Exception, psycopg2.DatabaseError) as error:\n",
    "                print(\"Error in transction Reverting all other operations of a transction \", error)\n",
    "    \n",
    "cursor.close()\n",
    "print(\"Transaction completed successfully \")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
